Skip to content

[ ![logo](../../../img/logo-white.svg) ](../../.. "PydanticAI")

PydanticAI

pydantic_ai.models.openai

Type to start searching

[ pydantic/pydantic-ai

  * v0.0.50
  * 8k
  * 687

](https://github.com/pydantic/pydantic-ai "Go to repository")

[ ![logo](../../../img/logo-white.svg) ](../../.. "PydanticAI") PydanticAI

[ pydantic/pydantic-ai

  * v0.0.50
  * 8k
  * 687

](https://github.com/pydantic/pydantic-ai "Go to repository")

  * [ Introduction  ](../../..)
  * [ Installation  ](../../../install/)
  * [ Getting Help  ](../../../help/)
  * [ Contributing  ](../../../contributing/)
  * [ Troubleshooting  ](../../../troubleshooting/)
  * Documentation  Documentation 
    * [ Agents  ](../../../agents/)
    * [ Models  ](../../../models/)
    * [ Dependencies  ](../../../dependencies/)
    * [ Function Tools  ](../../../tools/)
    * [ Common Tools  ](../../../common-tools/)
    * [ Results  ](../../../results/)
    * [ Messages and chat history  ](../../../message-history/)
    * [ Unit testing  ](../../../testing/)
    * [ Debugging and Monitoring  ](../../../logfire/)
    * [ Multi-agent Applications  ](../../../multi-agent-applications/)
    * [ Graphs  ](../../../graph/)
    * [ Evals  ](../../../evals/)
    * [ Image, Audio & Document Input  ](../../../input/)
    * [ MCP  ](../../../mcp/)

MCP

      * [ Client  ](../../../mcp/client/)
      * [ Server  ](../../../mcp/server/)
      * [ MCP Run Python  ](../../../mcp/run-python/)
    * [ Command Line Interface (CLI)  ](../../../cli/)
  * [ Examples  ](../../../examples/)

Examples

    * [ Pydantic Model  ](../../../examples/pydantic-model/)
    * [ Weather agent  ](../../../examples/weather-agent/)
    * [ Bank support  ](../../../examples/bank-support/)
    * [ SQL Generation  ](../../../examples/sql-gen/)
    * [ Flight booking  ](../../../examples/flight-booking/)
    * [ RAG  ](../../../examples/rag/)
    * [ Stream markdown  ](../../../examples/stream-markdown/)
    * [ Stream whales  ](../../../examples/stream-whales/)
    * [ Chat App with FastAPI  ](../../../examples/chat-app/)
    * [ Question Graph  ](../../../examples/question-graph/)
  * API Reference  API Reference 
    * [ pydantic_ai.agent  ](../../agent/)
    * [ pydantic_ai.tools  ](../../tools/)
    * [ pydantic_ai.common_tools  ](../../common_tools/)
    * [ pydantic_ai.result  ](../../result/)
    * [ pydantic_ai.messages  ](../../messages/)
    * [ pydantic_ai.exceptions  ](../../exceptions/)
    * [ pydantic_ai.settings  ](../../settings/)
    * [ pydantic_ai.usage  ](../../usage/)
    * [ pydantic_ai.mcp  ](../../mcp/)
    * [ pydantic_ai.format_as_xml  ](../../format_as_xml/)
    * [ pydantic_ai.models  ](../base/)
    * pydantic_ai.models.openai  [ pydantic_ai.models.openai  ](./) Table of contents 
      * Setup 
        * openai 
        * OpenAIModelName 
        * OpenAIModelSettings 
          * openai_reasoning_effort 
          * openai_user 
        * OpenAIResponsesModelSettings 
          * openai_builtin_tools 
          * openai_reasoning_generate_summary 
          * openai_truncation 
        * OpenAIModel 
          * __init__ 
          * model_name 
          * system 
        * OpenAIResponsesModel 
          * __init__ 
          * model_name 
          * system 
    * [ pydantic_ai.models.anthropic  ](../anthropic/)
    * [ pydantic_ai.models.bedrock  ](../bedrock/)
    * [ pydantic_ai.models.cohere  ](../cohere/)
    * [ pydantic_ai.models.gemini  ](../gemini/)
    * [ pydantic_ai.models.groq  ](../groq/)
    * [ pydantic_ai.models.instrumented  ](../instrumented/)
    * [ pydantic_ai.models.mistral  ](../mistral/)
    * [ pydantic_ai.models.test  ](../test/)
    * [ pydantic_ai.models.function  ](../function/)
    * [ pydantic_ai.models.fallback  ](../fallback/)
    * [ pydantic_ai.models.wrapper  ](../wrapper/)
    * [ pydantic_ai.providers  ](../../providers/)
    * [ pydantic_graph  ](../../pydantic_graph/graph/)
    * [ pydantic_graph.nodes  ](../../pydantic_graph/nodes/)
    * [ pydantic_graph.persistence  ](../../pydantic_graph/persistence/)
    * [ pydantic_graph.mermaid  ](../../pydantic_graph/mermaid/)
    * [ pydantic_graph.exceptions  ](../../pydantic_graph/exceptions/)
    * [ pydantic_evals.dataset  ](../../pydantic_evals/dataset/)
    * [ pydantic_evals.evaluators  ](../../pydantic_evals/evaluators/)
    * [ pydantic_evals.reporting  ](../../pydantic_evals/reporting/)
    * [ pydantic_evals.otel  ](../../pydantic_evals/otel/)
    * [ pydantic_evals.generation  ](../../pydantic_evals/generation/)

Table of contents

  * Setup 
    * openai 
    * OpenAIModelName 
    * OpenAIModelSettings 
      * openai_reasoning_effort 
      * openai_user 
    * OpenAIResponsesModelSettings 
      * openai_builtin_tools 
      * openai_reasoning_generate_summary 
      * openai_truncation 
    * OpenAIModel 
      * __init__ 
      * model_name 
      * system 
    * OpenAIResponsesModel 
      * __init__ 
      * model_name 
      * system 

# `pydantic_ai.models.openai`

## Setup

For details on how to set up authentication with this model, see [model
configuration for OpenAI](../../../models/#openai).

###  OpenAIModelName `module-attribute`

    
    
    OpenAIModelName = [Union](https://docs.python.org/3/library/typing.html#typing.Union "typing.Union")[[str](https://docs.python.org/3/library/stdtypes.html#str), ChatModel]
    

Possible OpenAI model names.

Since OpenAI supports a variety of date-stamped models, we explicitly list the
latest models but allow any name in the type hints. See [the OpenAI
docs](https://platform.openai.com/docs/models) for a full list.

Using this more broad type for the model name instead of the ChatModel
definition allows this model to be used more easily with other model types
(ie, Ollama, Deepseek).

###  OpenAIModelSettings

Bases: `[ModelSettings](../../settings/#pydantic_ai.settings.ModelSettings
"pydantic_ai.settings.ModelSettings")`

Settings used for an OpenAI model request.

ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

Source code in `pydantic_ai_slim/pydantic_ai/models/openai.py`

    
    
     89
     90
     91
     92
     93
     94
     95
     96
     97
     98
     99
    100
    101
    102
    103
    104
    105
    106

|

    
    
    class OpenAIModelSettings(ModelSettings, total=False):
        """Settings used for an OpenAI model request.
    
        ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.
        """
    
        openai_reasoning_effort: ReasoningEffort
        """Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
    
        Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
        result in faster responses and fewer tokens used on reasoning in a response.
        """
    
        openai_user: str
        """A unique identifier representing the end-user, which can help OpenAI monitor and detect abuse.
    
        See [OpenAI's safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids) for more details.
        """
      
  
---|---  
  
####  openai_reasoning_effort `instance-attribute`

    
    
    openai_reasoning_effort: ReasoningEffort
    

Constrains effort on reasoning for [reasoning
models](https://platform.openai.com/docs/guides/reasoning).

Currently supported values are `low`, `medium`, and `high`. Reducing reasoning
effort can result in faster responses and fewer tokens used on reasoning in a
response.

####  openai_user `instance-attribute`

    
    
    openai_user: [str](https://docs.python.org/3/library/stdtypes.html#str)
    

A unique identifier representing the end-user, which can help OpenAI monitor
and detect abuse.

See [OpenAI's safety best
practices](https://platform.openai.com/docs/guides/safety-best-practices#end-
user-ids) for more details.

###  OpenAIResponsesModelSettings

Bases: `OpenAIModelSettings`

Settings used for an OpenAI Responses model request.

ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

Source code in `pydantic_ai_slim/pydantic_ai/models/openai.py`

    
    
    109
    110
    111
    112
    113
    114
    115
    116
    117
    118
    119
    120
    121
    122
    123
    124
    125
    126
    127
    128
    129
    130
    131
    132
    133
    134
    135
    136
    137
    138
    139
    140

|

    
    
    class OpenAIResponsesModelSettings(OpenAIModelSettings, total=False):
        """Settings used for an OpenAI Responses model request.
    
        ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.
        """
    
        openai_builtin_tools: Sequence[FileSearchToolParam | WebSearchToolParam | ComputerToolParam]
        """The provided OpenAI built-in tools to use.
    
        See [OpenAI's built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) for more details.
        """
    
        openai_reasoning_generate_summary: Literal['detailed', 'concise']
        """A summary of the reasoning performed by the model.
    
        This can be useful for debugging and understanding the model's reasoning process.
        One of `concise` or `detailed`.
    
        Check the [OpenAI Computer use documentation](https://platform.openai.com/docs/guides/tools-computer-use#1-send-a-request-to-the-model)
        for more details.
        """
    
        openai_truncation: Literal['disabled', 'auto']
        """The truncation strategy to use for the model response.
    
        It can be either:
        - `disabled` (default): If a model response will exceed the context window size for a model, the
            request will fail with a 400 error.
        - `auto`: If the context of this response and previous ones exceeds the model's context window size,
            the model will truncate the response to fit the context window by dropping input items in the
            middle of the conversation.
        """
      
  
---|---  
  
####  openai_builtin_tools `instance-attribute`

    
    
    openai_builtin_tools: [Sequence](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence "collections.abc.Sequence")[
        FileSearchToolParam
        | WebSearchToolParam
        | ComputerToolParam
    ]
    

The provided OpenAI built-in tools to use.

See [OpenAI's built-in
tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) for
more details.

####  openai_reasoning_generate_summary `instance-attribute`

    
    
    openai_reasoning_generate_summary: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "typing.Literal")[
        "detailed", "concise"
    ]
    

A summary of the reasoning performed by the model.

This can be useful for debugging and understanding the model's reasoning
process. One of `concise` or `detailed`.

Check the [OpenAI Computer use
documentation](https://platform.openai.com/docs/guides/tools-computer-
use#1-send-a-request-to-the-model) for more details.

####  openai_truncation `instance-attribute`

    
    
    openai_truncation: [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "typing.Literal")['disabled', 'auto']
    

The truncation strategy to use for the model response.

It can be either: \- `disabled` (default): If a model response will exceed the
context window size for a model, the request will fail with a 400 error. \-
`auto`: If the context of this response and previous ones exceeds the model's
context window size, the model will truncate the response to fit the context
window by dropping input items in the middle of the conversation.

###  OpenAIModel `dataclass`

Bases: `[Model](../base/#pydantic_ai.models.Model "pydantic_ai.models.Model")`

A model that uses the OpenAI API.

Internally, this uses the [OpenAI Python
client](https://github.com/openai/openai-python) to interact with the API.

Apart from `__init__`, all methods are private or match those of the base
class.

Source code in `pydantic_ai_slim/pydantic_ai/models/openai.py`

    
    
    143
    144
    145
    146
    147
    148
    149
    150
    151
    152
    153
    154
    155
    156
    157
    158
    159
    160
    161
    162
    163
    164
    165
    166
    167
    168
    169
    170
    171
    172
    173
    174
    175
    176
    177
    178
    179
    180
    181
    182
    183
    184
    185
    186
    187
    188
    189
    190
    191
    192
    193
    194
    195
    196
    197
    198
    199
    200
    201
    202
    203
    204
    205
    206
    207
    208
    209
    210
    211
    212
    213
    214
    215
    216
    217
    218
    219
    220
    221
    222
    223
    224
    225
    226
    227
    228
    229
    230
    231
    232
    233
    234
    235
    236
    237
    238
    239
    240
    241
    242
    243
    244
    245
    246
    247
    248
    249
    250
    251
    252
    253
    254
    255
    256
    257
    258
    259
    260
    261
    262
    263
    264
    265
    266
    267
    268
    269
    270
    271
    272
    273
    274
    275
    276
    277
    278
    279
    280
    281
    282
    283
    284
    285
    286
    287
    288
    289
    290
    291
    292
    293
    294
    295
    296
    297
    298
    299
    300
    301
    302
    303
    304
    305
    306
    307
    308
    309
    310
    311
    312
    313
    314
    315
    316
    317
    318
    319
    320
    321
    322
    323
    324
    325
    326
    327
    328
    329
    330
    331
    332
    333
    334
    335
    336
    337
    338
    339
    340
    341
    342
    343
    344
    345
    346
    347
    348
    349
    350
    351
    352
    353
    354
    355
    356
    357
    358
    359
    360
    361
    362
    363
    364
    365
    366
    367
    368
    369
    370
    371
    372
    373
    374
    375
    376
    377
    378
    379
    380
    381
    382
    383
    384
    385
    386
    387
    388
    389
    390
    391
    392
    393
    394
    395
    396
    397
    398
    399
    400
    401
    402
    403
    404
    405
    406
    407
    408
    409
    410
    411
    412
    413
    414
    415
    416
    417
    418
    419
    420
    421
    422
    423
    424
    425
    426
    427
    428
    429
    430
    431
    432
    433
    434
    435
    436
    437
    438
    439
    440
    441
    442
    443
    444

|

    
    
    @dataclass(init=False)
    class OpenAIModel(Model):
        """A model that uses the OpenAI API.
    
        Internally, this uses the [OpenAI Python client](https://github.com/openai/openai-python) to interact with the API.
    
        Apart from `__init__`, all methods are private or match those of the base class.
        """
    
        client: AsyncOpenAI = field(repr=False)
        system_prompt_role: OpenAISystemPromptRole | None = field(default=None)
    
        _model_name: OpenAIModelName = field(repr=False)
        _system: str = field(default='openai', repr=False)
    
        def __init__(
            self,
            model_name: OpenAIModelName,
            *,
            provider: Literal['openai', 'deepseek', 'azure'] | Provider[AsyncOpenAI] = 'openai',
            system_prompt_role: OpenAISystemPromptRole | None = None,
        ):
            """Initialize an OpenAI model.
    
            Args:
                model_name: The name of the OpenAI model to use. List of model names available
                    [here](https://github.com/openai/openai-python/blob/v1.54.3/src/openai/types/chat_model.py#L7)
                    (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API).
                provider: The provider to use. Defaults to `'openai'`.
                system_prompt_role: The role to use for the system prompt message. If not provided, defaults to `'system'`.
                    In the future, this may be inferred from the model name.
            """
            self._model_name = model_name
            if isinstance(provider, str):
                provider = infer_provider(provider)
            self.client = provider.client
            self.system_prompt_role = system_prompt_role
    
        @property
        def base_url(self) -> str:
            return str(self.client.base_url)
    
        async def request(
            self,
            messages: list[ModelMessage],
            model_settings: ModelSettings | None,
            model_request_parameters: ModelRequestParameters,
        ) -> tuple[ModelResponse, usage.Usage]:
            check_allow_model_requests()
            response = await self._completions_create(
                messages, False, cast(OpenAIModelSettings, model_settings or {}), model_request_parameters
            )
            return self._process_response(response), _map_usage(response)
    
        @asynccontextmanager
        async def request_stream(
            self,
            messages: list[ModelMessage],
            model_settings: ModelSettings | None,
            model_request_parameters: ModelRequestParameters,
        ) -> AsyncIterator[StreamedResponse]:
            check_allow_model_requests()
            response = await self._completions_create(
                messages, True, cast(OpenAIModelSettings, model_settings or {}), model_request_parameters
            )
            async with response:
                yield await self._process_streamed_response(response)
    
        @property
        def model_name(self) -> OpenAIModelName:
            """The model name."""
            return self._model_name
    
        @property
        def system(self) -> str:
            """The system / model provider."""
            return self._system
    
        @overload
        async def _completions_create(
            self,
            messages: list[ModelMessage],
            stream: Literal[True],
            model_settings: OpenAIModelSettings,
            model_request_parameters: ModelRequestParameters,
        ) -> AsyncStream[ChatCompletionChunk]: ...
    
        @overload
        async def _completions_create(
            self,
            messages: list[ModelMessage],
            stream: Literal[False],
            model_settings: OpenAIModelSettings,
            model_request_parameters: ModelRequestParameters,
        ) -> chat.ChatCompletion: ...
    
        async def _completions_create(
            self,
            messages: list[ModelMessage],
            stream: bool,
            model_settings: OpenAIModelSettings,
            model_request_parameters: ModelRequestParameters,
        ) -> chat.ChatCompletion | AsyncStream[ChatCompletionChunk]:
            tools = self._get_tools(model_request_parameters)
    
            # standalone function to make it easier to override
            if not tools:
                tool_choice: Literal['none', 'required', 'auto'] | None = None
            elif not model_request_parameters.allow_text_result:
                tool_choice = 'required'
            else:
                tool_choice = 'auto'
    
            openai_messages: list[chat.ChatCompletionMessageParam] = []
            for m in messages:
                async for msg in self._map_message(m):
                    openai_messages.append(msg)
    
            try:
                return await self.client.chat.completions.create(
                    model=self._model_name,
                    messages=openai_messages,
                    n=1,
                    parallel_tool_calls=model_settings.get('parallel_tool_calls', NOT_GIVEN),
                    tools=tools or NOT_GIVEN,
                    tool_choice=tool_choice or NOT_GIVEN,
                    stream=stream,
                    stream_options={'include_usage': True} if stream else NOT_GIVEN,
                    max_completion_tokens=model_settings.get('max_tokens', NOT_GIVEN),
                    temperature=model_settings.get('temperature', NOT_GIVEN),
                    top_p=model_settings.get('top_p', NOT_GIVEN),
                    timeout=model_settings.get('timeout', NOT_GIVEN),
                    seed=model_settings.get('seed', NOT_GIVEN),
                    presence_penalty=model_settings.get('presence_penalty', NOT_GIVEN),
                    frequency_penalty=model_settings.get('frequency_penalty', NOT_GIVEN),
                    logit_bias=model_settings.get('logit_bias', NOT_GIVEN),
                    reasoning_effort=model_settings.get('openai_reasoning_effort', NOT_GIVEN),
                    user=model_settings.get('openai_user', NOT_GIVEN),
                )
            except APIStatusError as e:
                if (status_code := e.status_code) >= 400:
                    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e
                raise
    
        def _process_response(self, response: chat.ChatCompletion) -> ModelResponse:
            """Process a non-streamed response, and prepare a message to return."""
            timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)
            choice = response.choices[0]
            items: list[ModelResponsePart] = []
            if choice.message.content is not None:
                items.append(TextPart(choice.message.content))
            if choice.message.tool_calls is not None:
                for c in choice.message.tool_calls:
                    items.append(ToolCallPart(c.function.name, c.function.arguments, tool_call_id=c.id))
            return ModelResponse(items, model_name=response.model, timestamp=timestamp)
    
        async def _process_streamed_response(self, response: AsyncStream[ChatCompletionChunk]) -> OpenAIStreamedResponse:
            """Process a streamed response, and prepare a streaming response to return."""
            peekable_response = _utils.PeekableAsyncStream(response)
            first_chunk = await peekable_response.peek()
            if isinstance(first_chunk, _utils.Unset):
                raise UnexpectedModelBehavior('Streamed response ended without content or tool calls')
    
            return OpenAIStreamedResponse(
                _model_name=self._model_name,
                _response=peekable_response,
                _timestamp=datetime.fromtimestamp(first_chunk.created, tz=timezone.utc),
            )
    
        def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:
            tools = [self._map_tool_definition(r) for r in model_request_parameters.function_tools]
            if model_request_parameters.result_tools:
                tools += [self._map_tool_definition(r) for r in model_request_parameters.result_tools]
            return tools
    
        async def _map_message(self, message: ModelMessage) -> AsyncIterable[chat.ChatCompletionMessageParam]:
            """Just maps a `pydantic_ai.Message` to a `openai.types.ChatCompletionMessageParam`."""
            if isinstance(message, ModelRequest):
                async for item in self._map_user_message(message):
                    yield item
            elif isinstance(message, ModelResponse):
                texts: list[str] = []
                tool_calls: list[chat.ChatCompletionMessageToolCallParam] = []
                for item in message.parts:
                    if isinstance(item, TextPart):
                        texts.append(item.content)
                    elif isinstance(item, ToolCallPart):
                        tool_calls.append(self._map_tool_call(item))
                    else:
                        assert_never(item)
                message_param = chat.ChatCompletionAssistantMessageParam(role='assistant')
                if texts:
                    # Note: model responses from this model should only have one text item, so the following
                    # shouldn't merge multiple texts into one unless you switch models between runs:
                    message_param['content'] = '\n\n'.join(texts)
                if tool_calls:
                    message_param['tool_calls'] = tool_calls
                yield message_param
            else:
                assert_never(message)
    
        @staticmethod
        def _map_tool_call(t: ToolCallPart) -> chat.ChatCompletionMessageToolCallParam:
            return chat.ChatCompletionMessageToolCallParam(
                id=_guard_tool_call_id(t=t),
                type='function',
                function={'name': t.tool_name, 'arguments': t.args_as_json_str()},
            )
    
        @staticmethod
        def _map_tool_definition(f: ToolDefinition) -> chat.ChatCompletionToolParam:
            return {
                'type': 'function',
                'function': {
                    'name': f.name,
                    'description': f.description,
                    'parameters': f.parameters_json_schema,
                },
            }
    
        async def _map_user_message(self, message: ModelRequest) -> AsyncIterable[chat.ChatCompletionMessageParam]:
            for part in message.parts:
                if isinstance(part, SystemPromptPart):
                    if self.system_prompt_role == 'developer':
                        yield chat.ChatCompletionDeveloperMessageParam(role='developer', content=part.content)
                    elif self.system_prompt_role == 'user':
                        yield chat.ChatCompletionUserMessageParam(role='user', content=part.content)
                    else:
                        yield chat.ChatCompletionSystemMessageParam(role='system', content=part.content)
                elif isinstance(part, UserPromptPart):
                    yield await self._map_user_prompt(part)
                elif isinstance(part, ToolReturnPart):
                    yield chat.ChatCompletionToolMessageParam(
                        role='tool',
                        tool_call_id=_guard_tool_call_id(t=part),
                        content=part.model_response_str(),
                    )
                elif isinstance(part, RetryPromptPart):
                    if part.tool_name is None:
                        yield chat.ChatCompletionUserMessageParam(role='user', content=part.model_response())
                    else:
                        yield chat.ChatCompletionToolMessageParam(
                            role='tool',
                            tool_call_id=_guard_tool_call_id(t=part),
                            content=part.model_response(),
                        )
                else:
                    assert_never(part)
    
        @staticmethod
        async def _map_user_prompt(part: UserPromptPart) -> chat.ChatCompletionUserMessageParam:
            content: str | list[ChatCompletionContentPartParam]
            if isinstance(part.content, str):
                content = part.content
            else:
                content = []
                for item in part.content:
                    if isinstance(item, str):
                        content.append(ChatCompletionContentPartTextParam(text=item, type='text'))
                    elif isinstance(item, ImageUrl):
                        image_url = ImageURL(url=item.url)
                        content.append(ChatCompletionContentPartImageParam(image_url=image_url, type='image_url'))
                    elif isinstance(item, BinaryContent):
                        base64_encoded = base64.b64encode(item.data).decode('utf-8')
                        if item.is_image:
                            image_url = ImageURL(url=f'data:{item.media_type};base64,{base64_encoded}')
                            content.append(ChatCompletionContentPartImageParam(image_url=image_url, type='image_url'))
                        elif item.is_audio:
                            assert item.format in ('wav', 'mp3')
                            audio = InputAudio(data=base64_encoded, format=item.format)
                            content.append(ChatCompletionContentPartInputAudioParam(input_audio=audio, type='input_audio'))
                        else:  # pragma: no cover
                            raise RuntimeError(f'Unsupported binary content type: {item.media_type}')
                    elif isinstance(item, AudioUrl):  # pragma: no cover
                        client = cached_async_http_client()
                        response = await client.get(item.url)
                        response.raise_for_status()
                        base64_encoded = base64.b64encode(response.content).decode('utf-8')
                        audio = InputAudio(data=base64_encoded, format=response.headers.get('content-type'))
                        content.append(ChatCompletionContentPartInputAudioParam(input_audio=audio, type='input_audio'))
                    elif isinstance(item, DocumentUrl):  # pragma: no cover
                        raise NotImplementedError('DocumentUrl is not supported for OpenAI')
                        # The following implementation should have worked, but it seems we have the following error:
                        # pydantic_ai.exceptions.ModelHTTPError: status_code: 400, model_name: gpt-4o, body:
                        # {
                        #   'message': "Unknown parameter: 'messages[1].content[1].file.data'.",
                        #   'type': 'invalid_request_error',
                        #   'param': 'messages[1].content[1].file.data',
                        #   'code': 'unknown_parameter'
                        # }
                        #
                        # client = cached_async_http_client()
                        # response = await client.get(item.url)
                        # response.raise_for_status()
                        # base64_encoded = base64.b64encode(response.content).decode('utf-8')
                        # media_type = response.headers.get('content-type').split(';')[0]
                        # file_data = f'data:{media_type};base64,{base64_encoded}'
                        # file = File(file={'file_data': file_data, 'file_name': item.url, 'file_id': item.url}, type='file')
                        # content.append(file)
                    else:
                        assert_never(item)
            return chat.ChatCompletionUserMessageParam(role='user', content=content)
      
  
---|---  
  
####  __init__

    
    
    __init__(
        model_name: OpenAIModelName,
        *,
        provider: (
            [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "typing.Literal")["openai", "deepseek", "azure"]
            | [Provider](../../providers/#pydantic_ai.providers.Provider "pydantic_ai.providers.Provider")[AsyncOpenAI]
        ) = "openai",
        system_prompt_role: OpenAISystemPromptRole | None = None
    )
    

Initialize an OpenAI model.

Parameters:

Name | Type | Description | Default  
---|---|---|---  
`model_name` |  `OpenAIModelName` |  The name of the OpenAI model to use. List of model names available [here](https://github.com/openai/openai-python/blob/v1.54.3/src/openai/types/chat_model.py#L7) (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API). |  _required_  
`provider` |  `[Literal](https://docs.python.org/3/library/typing.html#typing.Literal "typing.Literal")['openai', 'deepseek', 'azure'] | [Provider](../../providers/#pydantic_ai.providers.Provider "pydantic_ai.providers.Provider")[AsyncOpenAI]` |  The provider to use. Defaults to `'openai'`. |  `'openai'`  
`system_prompt_role` |  `OpenAISystemPromptRole | None` |  The role to use for the system prompt message. If not provided, defaults to `'system'`. In the future, this may be inferred from the model name. |  `None`  
  
Source code in `pydantic_ai_slim/pydantic_ai/models/openai.py`

    
    
    158
    159
    160
    161
    162
    163
    164
    165
    166
    167
    168
    169
    170
    171
    172
    173
    174
    175
    176
    177
    178
    179

|

    
    
    def __init__(
        self,
        model_name: OpenAIModelName,
        *,
        provider: Literal['openai', 'deepseek', 'azure'] | Provider[AsyncOpenAI] = 'openai',
        system_prompt_role: OpenAISystemPromptRole | None = None,
    ):
        """Initialize an OpenAI model.
    
        Args:
            model_name: The name of the OpenAI model to use. List of model names available
                [here](https://github.com/openai/openai-python/blob/v1.54.3/src/openai/types/chat_model.py#L7)
                (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API).
            provider: The provider to use. Defaults to `'openai'`.
            system_prompt_role: The role to use for the system prompt message. If not provided, defaults to `'system'`.
                In the future, this may be inferred from the model name.
        """
        self._model_name = model_name
        if isinstance(provider, str):
            provider = infer_provider(provider)
        self.client = provider.client
        self.system_prompt_role = system_prompt_role
      
  
---|---  
  
####  model_name `property`

    
    
    model_name: OpenAIModelName
    

The model name.

####  system `property`

    
    
    system: [str](https://docs.python.org/3/library/stdtypes.html#str)
    

The system / model provider.

###  OpenAIResponsesModel `dataclass`

Bases: `[Model](../base/#pydantic_ai.models.Model "pydantic_ai.models.Model")`

A model that uses the OpenAI Responses API.

The [OpenAI Responses API](https://platform.openai.com/docs/api-
reference/responses) is the new API for OpenAI models.

The Responses API has built-in tools, that you can use instead of building
your own:

  * [Web search](https://platform.openai.com/docs/guides/tools-web-search)
  * [File search](https://platform.openai.com/docs/guides/tools-file-search)
  * [Computer use](https://platform.openai.com/docs/guides/tools-computer-use)

Use the `openai_builtin_tools` setting to add these tools to your model.

If you are interested in the differences between the Responses API and the
Chat Completions API, see the [OpenAI API
docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions).

Source code in `pydantic_ai_slim/pydantic_ai/models/openai.py`

    
    
    447
    448
    449
    450
    451
    452
    453
    454
    455
    456
    457
    458
    459
    460
    461
    462
    463
    464
    465
    466
    467
    468
    469
    470
    471
    472
    473
    474
    475
    476
    477
    478
    479
    480
    481
    482
    483
    484
    485
    486
    487
    488
    489
    490
    491
    492
    493
    494
    495
    496
    497
    498
    499
    500
    501
    502
    503
    504
    505
    506
    507
    508
    509
    510
    511
    512
    513
    514
    515
    516
    517
    518
    519
    520
    521
    522
    523
    524
    525
    526
    527
    528
    529
    530
    531
    532
    533
    534
    535
    536
    537
    538
    539
    540
    541
    542
    543
    544
    545
    546
    547
    548
    549
    550
    551
    552
    553
    554
    555
    556
    557
    558
    559
    560
    561
    562
    563
    564
    565
    566
    567
    568
    569
    570
    571
    572
    573
    574
    575
    576
    577
    578
    579
    580
    581
    582
    583
    584
    585
    586
    587
    588
    589
    590
    591
    592
    593
    594
    595
    596
    597
    598
    599
    600
    601
    602
    603
    604
    605
    606
    607
    608
    609
    610
    611
    612
    613
    614
    615
    616
    617
    618
    619
    620
    621
    622
    623
    624
    625
    626
    627
    628
    629
    630
    631
    632
    633
    634
    635
    636
    637
    638
    639
    640
    641
    642
    643
    644
    645
    646
    647
    648
    649
    650
    651
    652
    653
    654
    655
    656
    657
    658
    659
    660
    661
    662
    663
    664
    665
    666
    667
    668
    669
    670
    671
    672
    673
    674
    675
    676
    677
    678
    679
    680
    681
    682
    683
    684
    685
    686
    687
    688
    689
    690
    691
    692
    693
    694
    695
    696
    697
    698
    699
    700
    701
    702
    703
    704
    705
    706
    707
    708
    709
    710
    711
    712
    713
    714
    715
    716
    717
    718
    719
    720
    721
    722
    723
    724
    725
    726
    727
    728
    729
    730
    731
    732
    733
    734
    735
    736
    737
    738
    739
    740
    741
    742
    743
    744
    745
    746
    747
    748
    749
    750
    751
    752
    753
    754
    755
    756

|

    
    
    @dataclass(init=False)
    class OpenAIResponsesModel(Model):
        """A model that uses the OpenAI Responses API.
    
        The [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) is the
        new API for OpenAI models.
    
        The Responses API has built-in tools, that you can use instead of building your own:
    
        - [Web search](https://platform.openai.com/docs/guides/tools-web-search)
        - [File search](https://platform.openai.com/docs/guides/tools-file-search)
        - [Computer use](https://platform.openai.com/docs/guides/tools-computer-use)
    
        Use the `openai_builtin_tools` setting to add these tools to your model.
    
        If you are interested in the differences between the Responses API and the Chat Completions API,
        see the [OpenAI API docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions).
        """
    
        client: AsyncOpenAI = field(repr=False)
        system_prompt_role: OpenAISystemPromptRole | None = field(default=None)
    
        _model_name: OpenAIModelName = field(repr=False)
        _system: str = field(default='openai', repr=False)
    
        def __init__(
            self,
            model_name: OpenAIModelName,
            *,
            provider: Literal['openai', 'deepseek', 'azure'] | Provider[AsyncOpenAI] = 'openai',
        ):
            """Initialize an OpenAI Responses model.
    
            Args:
                model_name: The name of the OpenAI model to use.
                provider: The provider to use. Defaults to `'openai'`.
            """
            self._model_name = model_name
            if isinstance(provider, str):
                provider = infer_provider(provider)
            self.client = provider.client
    
        @property
        def model_name(self) -> OpenAIModelName:
            """The model name."""
            return self._model_name
    
        @property
        def system(self) -> str:
            """The system / model provider."""
            return self._system
    
        async def request(
            self,
            messages: list[ModelRequest | ModelResponse],
            model_settings: ModelSettings | None,
            model_request_parameters: ModelRequestParameters,
        ) -> tuple[ModelResponse, usage.Usage]:
            check_allow_model_requests()
            response = await self._responses_create(
                messages, False, cast(OpenAIResponsesModelSettings, model_settings or {}), model_request_parameters
            )
            return self._process_response(response), _map_usage(response)
    
        @asynccontextmanager
        async def request_stream(
            self,
            messages: list[ModelMessage],
            model_settings: ModelSettings | None,
            model_request_parameters: ModelRequestParameters,
        ) -> AsyncIterator[StreamedResponse]:
            check_allow_model_requests()
            response = await self._responses_create(
                messages, True, cast(OpenAIResponsesModelSettings, model_settings or {}), model_request_parameters
            )
            async with response:
                yield await self._process_streamed_response(response)
    
        def _process_response(self, response: responses.Response) -> ModelResponse:
            """Process a non-streamed response, and prepare a message to return."""
            timestamp = datetime.fromtimestamp(response.created_at, tz=timezone.utc)
            items: list[ModelResponsePart] = []
            items.append(TextPart(response.output_text))
            for item in response.output:
                if item.type == 'function_call':
                    items.append(ToolCallPart(item.name, item.arguments, tool_call_id=item.call_id))
            return ModelResponse(items, model_name=response.model, timestamp=timestamp)
    
        async def _process_streamed_response(
            self, response: AsyncStream[responses.ResponseStreamEvent]
        ) -> OpenAIResponsesStreamedResponse:
            """Process a streamed response, and prepare a streaming response to return."""
            peekable_response = _utils.PeekableAsyncStream(response)
            first_chunk = await peekable_response.peek()
            if isinstance(first_chunk, _utils.Unset):  # pragma: no cover
                raise UnexpectedModelBehavior('Streamed response ended without content or tool calls')
    
            assert isinstance(first_chunk, responses.ResponseCreatedEvent)
            return OpenAIResponsesStreamedResponse(
                _model_name=self._model_name,
                _response=peekable_response,
                _timestamp=datetime.fromtimestamp(first_chunk.response.created_at, tz=timezone.utc),
            )
    
        @overload
        async def _responses_create(
            self,
            messages: list[ModelRequest | ModelResponse],
            stream: Literal[False],
            model_settings: OpenAIResponsesModelSettings,
            model_request_parameters: ModelRequestParameters,
        ) -> responses.Response: ...
    
        @overload
        async def _responses_create(
            self,
            messages: list[ModelRequest | ModelResponse],
            stream: Literal[True],
            model_settings: OpenAIResponsesModelSettings,
            model_request_parameters: ModelRequestParameters,
        ) -> AsyncStream[responses.ResponseStreamEvent]: ...
    
        async def _responses_create(
            self,
            messages: list[ModelRequest | ModelResponse],
            stream: bool,
            model_settings: OpenAIResponsesModelSettings,
            model_request_parameters: ModelRequestParameters,
        ) -> responses.Response | AsyncStream[responses.ResponseStreamEvent]:
            tools = self._get_tools(model_request_parameters)
            tools = list(model_settings.get('openai_builtin_tools', [])) + tools
    
            # standalone function to make it easier to override
            if not tools:
                tool_choice: Literal['none', 'required', 'auto'] | None = None
            elif not model_request_parameters.allow_text_result:
                tool_choice = 'required'
            else:
                tool_choice = 'auto'
    
            system_prompt, openai_messages = await self._map_message(messages)
            reasoning = self._get_reasoning(model_settings)
    
            try:
                return await self.client.responses.create(
                    input=openai_messages,
                    model=self._model_name,
                    instructions=system_prompt,
                    parallel_tool_calls=model_settings.get('parallel_tool_calls', NOT_GIVEN),
                    tools=tools or NOT_GIVEN,
                    tool_choice=tool_choice or NOT_GIVEN,
                    max_output_tokens=model_settings.get('max_tokens', NOT_GIVEN),
                    stream=stream,
                    temperature=model_settings.get('temperature', NOT_GIVEN),
                    top_p=model_settings.get('top_p', NOT_GIVEN),
                    truncation=model_settings.get('openai_truncation', NOT_GIVEN),
                    timeout=model_settings.get('timeout', NOT_GIVEN),
                    reasoning=reasoning,
                    user=model_settings.get('user', NOT_GIVEN),
                )
            except APIStatusError as e:
                if (status_code := e.status_code) >= 400:
                    raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e
                raise
    
        def _get_reasoning(self, model_settings: OpenAIResponsesModelSettings) -> Reasoning | NotGiven:
            reasoning_effort = model_settings.get('openai_reasoning_effort', None)
            reasoning_generate_summary = model_settings.get('openai_reasoning_generate_summary', None)
    
            if reasoning_effort is None and reasoning_generate_summary is None:
                return NOT_GIVEN
            return Reasoning(effort=reasoning_effort, generate_summary=reasoning_generate_summary)
    
        def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[responses.FunctionToolParam]:
            tools = [self._map_tool_definition(r) for r in model_request_parameters.function_tools]
            if model_request_parameters.result_tools:
                tools += [self._map_tool_definition(r) for r in model_request_parameters.result_tools]
            return tools
    
        @staticmethod
        def _map_tool_definition(f: ToolDefinition) -> responses.FunctionToolParam:
            return {
                'name': f.name,
                'parameters': f.parameters_json_schema,
                'type': 'function',
                'description': f.description,
                'strict': True,
            }
    
        async def _map_message(self, messages: list[ModelMessage]) -> tuple[str, list[responses.ResponseInputItemParam]]:
            """Just maps a `pydantic_ai.Message` to a `openai.types.responses.ResponseInputParam`."""
            system_prompt: str = ''
            openai_messages: list[responses.ResponseInputItemParam] = []
            for message in messages:
                if isinstance(message, ModelRequest):
                    for part in message.parts:
                        if isinstance(part, SystemPromptPart):
                            system_prompt += part.content
                        elif isinstance(part, UserPromptPart):
                            openai_messages.append(await self._map_user_prompt(part))
                        elif isinstance(part, ToolReturnPart):
                            openai_messages.append(
                                FunctionCallOutput(
                                    type='function_call_output',
                                    call_id=_guard_tool_call_id(t=part),
                                    output=part.model_response_str(),
                                )
                            )
                        elif isinstance(part, RetryPromptPart):
                            # TODO(Marcelo): How do we test this conditional branch?
                            if part.tool_name is None:  # pragma: no cover
                                openai_messages.append(
                                    Message(role='user', content=[{'type': 'input_text', 'text': part.model_response()}])
                                )
                            else:
                                openai_messages.append(
                                    FunctionCallOutput(
                                        type='function_call_output',
                                        call_id=_guard_tool_call_id(t=part),
                                        output=part.model_response(),
                                    )
                                )
                        else:
                            assert_never(part)
                elif isinstance(message, ModelResponse):
                    for item in message.parts:
                        if isinstance(item, TextPart):
                            openai_messages.append(responses.EasyInputMessageParam(role='assistant', content=item.content))
                        elif isinstance(item, ToolCallPart):
                            openai_messages.append(self._map_tool_call(item))
                        else:
                            assert_never(item)
                else:
                    assert_never(message)
            return system_prompt, openai_messages
    
        @staticmethod
        def _map_tool_call(t: ToolCallPart) -> responses.ResponseFunctionToolCallParam:
            return responses.ResponseFunctionToolCallParam(
                arguments=t.args_as_json_str(),
                call_id=_guard_tool_call_id(t=t),
                name=t.tool_name,
                type='function_call',
            )
    
        @staticmethod
        async def _map_user_prompt(part: UserPromptPart) -> responses.EasyInputMessageParam:
            content: str | list[responses.ResponseInputContentParam]
            if isinstance(part.content, str):
                content = part.content
            else:
                content = []
                for item in part.content:
                    if isinstance(item, str):
                        content.append(responses.ResponseInputTextParam(text=item, type='input_text'))
                    elif isinstance(item, BinaryContent):
                        base64_encoded = base64.b64encode(item.data).decode('utf-8')
                        if item.is_image:
                            content.append(
                                responses.ResponseInputImageParam(
                                    image_url=f'data:{item.media_type};base64,{base64_encoded}',
                                    type='input_image',
                                    detail='auto',
                                )
                            )
                        elif item.is_document:
                            content.append(
                                responses.ResponseInputFileParam(
                                    type='input_file',
                                    file_data=f'data:{item.media_type};base64,{base64_encoded}',
                                    # NOTE: Type wise it's not necessary to include the filename, but it's required by the
                                    # API itself. If we add empty string, the server sends a 500 error - which OpenAI needs
                                    # to fix. In any case, we add a placeholder name.
                                    filename=f'filename.{item.format}',
                                )
                            )
                        elif item.is_audio:
                            raise NotImplementedError('Audio as binary content is not supported for OpenAI Responses API.')
                        else:  # pragma: no cover
                            raise RuntimeError(f'Unsupported binary content type: {item.media_type}')
                    elif isinstance(item, ImageUrl):
                        content.append(
                            responses.ResponseInputImageParam(image_url=item.url, type='input_image', detail='auto')
                        )
                    elif isinstance(item, AudioUrl):  # pragma: no cover
                        client = cached_async_http_client()
                        response = await client.get(item.url)
                        response.raise_for_status()
                        base64_encoded = base64.b64encode(response.content).decode('utf-8')
                        content.append(
                            responses.ResponseInputFileParam(
                                type='input_file',
                                file_data=f'data:{item.media_type};base64,{base64_encoded}',
                            )
                        )
                    elif isinstance(item, DocumentUrl):  # pragma: no cover
                        client = cached_async_http_client()
                        response = await client.get(item.url)
                        response.raise_for_status()
                        base64_encoded = base64.b64encode(response.content).decode('utf-8')
                        content.append(
                            responses.ResponseInputFileParam(
                                type='input_file',
                                file_data=f'data:{item.media_type};base64,{base64_encoded}',
                                filename=f'filename.{item.format}',
                            )
                        )
                    else:
                        assert_never(item)
            return responses.EasyInputMessageParam(role='user', content=content)
      
  
---|---  
  
####  __init__

    
    
    __init__(
        model_name: OpenAIModelName,
        *,
        provider: (
            [Literal](https://docs.python.org/3/library/typing.html#typing.Literal "typing.Literal")["openai", "deepseek", "azure"]
            | [Provider](../../providers/#pydantic_ai.providers.Provider "pydantic_ai.providers.Provider")[AsyncOpenAI]
        ) = "openai"
    )
    

Initialize an OpenAI Responses model.

Parameters:

Name | Type | Description | Default  
---|---|---|---  
`model_name` |  `OpenAIModelName` |  The name of the OpenAI model to use. |  _required_  
`provider` |  `[Literal](https://docs.python.org/3/library/typing.html#typing.Literal "typing.Literal")['openai', 'deepseek', 'azure'] | [Provider](../../providers/#pydantic_ai.providers.Provider "pydantic_ai.providers.Provider")[AsyncOpenAI]` |  The provider to use. Defaults to `'openai'`. |  `'openai'`  
  
Source code in `pydantic_ai_slim/pydantic_ai/models/openai.py`

    
    
    472
    473
    474
    475
    476
    477
    478
    479
    480
    481
    482
    483
    484
    485
    486
    487

|

    
    
    def __init__(
        self,
        model_name: OpenAIModelName,
        *,
        provider: Literal['openai', 'deepseek', 'azure'] | Provider[AsyncOpenAI] = 'openai',
    ):
        """Initialize an OpenAI Responses model.
    
        Args:
            model_name: The name of the OpenAI model to use.
            provider: The provider to use. Defaults to `'openai'`.
        """
        self._model_name = model_name
        if isinstance(provider, str):
            provider = infer_provider(provider)
        self.client = provider.client
      
  
---|---  
  
####  model_name `property`

    
    
    model_name: OpenAIModelName
    

The model name.

####  system `property`

    
    
    system: [str](https://docs.python.org/3/library/stdtypes.html#str)
    

The system / model provider.

 Pydantic Services Inc. 2024 to present

