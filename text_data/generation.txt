Skip to content

[ ![logo](../../../img/logo-white.svg) ](../../.. "PydanticAI")

PydanticAI

pydantic_evals.generation

Type to start searching

[ pydantic/pydantic-ai

  * v0.0.50
  * 8k
  * 687

](https://github.com/pydantic/pydantic-ai "Go to repository")

[ ![logo](../../../img/logo-white.svg) ](../../.. "PydanticAI") PydanticAI

[ pydantic/pydantic-ai

  * v0.0.50
  * 8k
  * 687

](https://github.com/pydantic/pydantic-ai "Go to repository")

  * [ Introduction  ](../../..)
  * [ Installation  ](../../../install/)
  * [ Getting Help  ](../../../help/)
  * [ Contributing  ](../../../contributing/)
  * [ Troubleshooting  ](../../../troubleshooting/)
  * Documentation  Documentation 
    * [ Agents  ](../../../agents/)
    * [ Models  ](../../../models/)
    * [ Dependencies  ](../../../dependencies/)
    * [ Function Tools  ](../../../tools/)
    * [ Common Tools  ](../../../common-tools/)
    * [ Results  ](../../../results/)
    * [ Messages and chat history  ](../../../message-history/)
    * [ Unit testing  ](../../../testing/)
    * [ Debugging and Monitoring  ](../../../logfire/)
    * [ Multi-agent Applications  ](../../../multi-agent-applications/)
    * [ Graphs  ](../../../graph/)
    * [ Evals  ](../../../evals/)
    * [ Image, Audio & Document Input  ](../../../input/)
    * [ MCP  ](../../../mcp/)

MCP

      * [ Client  ](../../../mcp/client/)
      * [ Server  ](../../../mcp/server/)
      * [ MCP Run Python  ](../../../mcp/run-python/)
    * [ Command Line Interface (CLI)  ](../../../cli/)
  * [ Examples  ](../../../examples/)

Examples

    * [ Pydantic Model  ](../../../examples/pydantic-model/)
    * [ Weather agent  ](../../../examples/weather-agent/)
    * [ Bank support  ](../../../examples/bank-support/)
    * [ SQL Generation  ](../../../examples/sql-gen/)
    * [ Flight booking  ](../../../examples/flight-booking/)
    * [ RAG  ](../../../examples/rag/)
    * [ Stream markdown  ](../../../examples/stream-markdown/)
    * [ Stream whales  ](../../../examples/stream-whales/)
    * [ Chat App with FastAPI  ](../../../examples/chat-app/)
    * [ Question Graph  ](../../../examples/question-graph/)
  * API Reference  API Reference 
    * [ pydantic_ai.agent  ](../../agent/)
    * [ pydantic_ai.tools  ](../../tools/)
    * [ pydantic_ai.common_tools  ](../../common_tools/)
    * [ pydantic_ai.result  ](../../result/)
    * [ pydantic_ai.messages  ](../../messages/)
    * [ pydantic_ai.exceptions  ](../../exceptions/)
    * [ pydantic_ai.settings  ](../../settings/)
    * [ pydantic_ai.usage  ](../../usage/)
    * [ pydantic_ai.mcp  ](../../mcp/)
    * [ pydantic_ai.format_as_xml  ](../../format_as_xml/)
    * [ pydantic_ai.models  ](../../models/base/)
    * [ pydantic_ai.models.openai  ](../../models/openai/)
    * [ pydantic_ai.models.anthropic  ](../../models/anthropic/)
    * [ pydantic_ai.models.bedrock  ](../../models/bedrock/)
    * [ pydantic_ai.models.cohere  ](../../models/cohere/)
    * [ pydantic_ai.models.gemini  ](../../models/gemini/)
    * [ pydantic_ai.models.groq  ](../../models/groq/)
    * [ pydantic_ai.models.instrumented  ](../../models/instrumented/)
    * [ pydantic_ai.models.mistral  ](../../models/mistral/)
    * [ pydantic_ai.models.test  ](../../models/test/)
    * [ pydantic_ai.models.function  ](../../models/function/)
    * [ pydantic_ai.models.fallback  ](../../models/fallback/)
    * [ pydantic_ai.models.wrapper  ](../../models/wrapper/)
    * [ pydantic_ai.providers  ](../../providers/)
    * [ pydantic_graph  ](../../pydantic_graph/graph/)
    * [ pydantic_graph.nodes  ](../../pydantic_graph/nodes/)
    * [ pydantic_graph.persistence  ](../../pydantic_graph/persistence/)
    * [ pydantic_graph.mermaid  ](../../pydantic_graph/mermaid/)
    * [ pydantic_graph.exceptions  ](../../pydantic_graph/exceptions/)
    * [ pydantic_evals.dataset  ](../dataset/)
    * [ pydantic_evals.evaluators  ](../evaluators/)
    * [ pydantic_evals.reporting  ](../reporting/)
    * [ pydantic_evals.otel  ](../otel/)
    * pydantic_evals.generation  [ pydantic_evals.generation  ](./) Table of contents 
      * generation 
      * generate_dataset 

Table of contents

  * generation 
  * generate_dataset 

# `pydantic_evals.generation`

Utilities for generating example datasets for pydantic_evals.

This module provides functions for generating sample datasets for testing and
examples, using LLMs to create realistic test data with proper structure.

###  generate_dataset `async`

    
    
    generate_dataset(
        *,
        dataset_type: [type](https://docs.python.org/3/library/functions.html#type)[
            [Dataset](../dataset/#pydantic_evals.dataset.Dataset "pydantic_evals.Dataset")[InputsT, OutputT, MetadataT]
        ],
        path: [Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "pathlib.Path") | [str](https://docs.python.org/3/library/stdtypes.html#str) | None = None,
        custom_evaluator_types: [Sequence](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence "collections.abc.Sequence")[
            [type](https://docs.python.org/3/library/functions.html#type)[[Evaluator](../evaluators/#pydantic_evals.evaluators.Evaluator "pydantic_evals.evaluators.evaluator.Evaluator")[InputsT, OutputT, MetadataT]]
        ] = (),
        model: [Model](../../models/base/#pydantic_ai.models.Model "pydantic_ai.models.Model") | [KnownModelName](../../models/base/#pydantic_ai.models.KnownModelName "pydantic_ai.models.KnownModelName") = "openai:gpt-4o",
        n_examples: [int](https://docs.python.org/3/library/functions.html#int) = 3,
        extra_instructions: [str](https://docs.python.org/3/library/stdtypes.html#str) | None = None
    ) -> [Dataset](../dataset/#pydantic_evals.dataset.Dataset "pydantic_evals.Dataset")[InputsT, OutputT, MetadataT]
    

Use an LLM to generate a dataset of test cases, each consisting of input,
expected output, and metadata.

This function creates a properly structured dataset with the specified input,
output, and metadata types. It uses an LLM to attempt to generate realistic
test cases that conform to the types' schemas.

Parameters:

Name | Type | Description | Default  
---|---|---|---  
`path` |  `[Path](https://docs.python.org/3/library/pathlib.html#pathlib.Path "pathlib.Path") | [str](https://docs.python.org/3/library/stdtypes.html#str) | None` |  Optional path to save the generated dataset. If provided, the dataset will be saved to this location. |  `None`  
`dataset_type` |  `[type](https://docs.python.org/3/library/functions.html#type)[[Dataset](../dataset/#pydantic_evals.dataset.Dataset "pydantic_evals.Dataset")[InputsT, OutputT, MetadataT]]` |  The type of dataset to generate, with the desired input, output, and metadata types. |  _required_  
`custom_evaluator_types` |  `[Sequence](https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence "collections.abc.Sequence")[[type](https://docs.python.org/3/library/functions.html#type)[[Evaluator](../evaluators/#pydantic_evals.evaluators.Evaluator "pydantic_evals.evaluators.evaluator.Evaluator")[InputsT, OutputT, MetadataT]]]` |  Optional sequence of custom evaluator classes to include in the schema. |  `()`  
`model` |  `[Model](../../models/base/#pydantic_ai.models.Model "pydantic_ai.models.Model") | [KnownModelName](../../models/base/#pydantic_ai.models.KnownModelName "pydantic_ai.models.KnownModelName")` |  The PydanticAI model to use for generation. Defaults to 'gpt-4o'. |  `'openai:gpt-4o'`  
`n_examples` |  `[int](https://docs.python.org/3/library/functions.html#int)` |  Number of examples to generate. Defaults to 3. |  `3`  
`extra_instructions` |  `[str](https://docs.python.org/3/library/stdtypes.html#str) | None` |  Optional additional instructions to provide to the LLM. |  `None`  
  
Returns:

Type | Description  
---|---  
`[Dataset](../dataset/#pydantic_evals.dataset.Dataset "pydantic_evals.Dataset")[InputsT, OutputT, MetadataT]` |  A properly structured Dataset object with generated test cases.  
  
Raises:

Type | Description  
---|---  
`ValidationError` |  If the LLM's response cannot be parsed as a valid dataset.  
  
Source code in `pydantic_evals/pydantic_evals/generation.py`

    
    
    32
    33
    34
    35
    36
    37
    38
    39
    40
    41
    42
    43
    44
    45
    46
    47
    48
    49
    50
    51
    52
    53
    54
    55
    56
    57
    58
    59
    60
    61
    62
    63
    64
    65
    66
    67
    68
    69
    70
    71
    72
    73
    74
    75
    76
    77
    78
    79
    80
    81
    82

|

    
    
    async def generate_dataset(
        *,
        dataset_type: type[Dataset[InputsT, OutputT, MetadataT]],
        path: Path | str | None = None,
        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),
        model: models.Model | models.KnownModelName = 'openai:gpt-4o',
        n_examples: int = 3,
        extra_instructions: str | None = None,
    ) -> Dataset[InputsT, OutputT, MetadataT]:  # pragma: no cover
        """Use an LLM to generate a dataset of test cases, each consisting of input, expected output, and metadata.
    
        This function creates a properly structured dataset with the specified input, output, and metadata types.
        It uses an LLM to attempt to generate realistic test cases that conform to the types' schemas.
    
        Args:
            path: Optional path to save the generated dataset. If provided, the dataset will be saved to this location.
            dataset_type: The type of dataset to generate, with the desired input, output, and metadata types.
            custom_evaluator_types: Optional sequence of custom evaluator classes to include in the schema.
            model: The PydanticAI model to use for generation. Defaults to 'gpt-4o'.
            n_examples: Number of examples to generate. Defaults to 3.
            extra_instructions: Optional additional instructions to provide to the LLM.
    
        Returns:
            A properly structured Dataset object with generated test cases.
    
        Raises:
            ValidationError: If the LLM's response cannot be parsed as a valid dataset.
        """
        result_schema = dataset_type.model_json_schema_with_evaluators(custom_evaluator_types)
    
        # TODO(DavidM): Update this once we add better response_format and/or ResultTool support to PydanticAI
        agent = Agent(
            model,
            system_prompt=(
                f'Generate an object that is in compliance with this JSON schema:\n{result_schema}\n\n'
                f'Include {n_examples} example cases.'
                ' You must not include any characters in your response before the opening { of the JSON object, or after the closing }.'
            ),
            result_type=str,
            retries=1,
        )
    
        result = await agent.run(extra_instructions or 'Please generate the object.')
        try:
            result = dataset_type.from_text(result.data, fmt='json', custom_evaluator_types=custom_evaluator_types)
        except ValidationError as e:
            print(f'Raw response from model:\n{result.data}')
            raise e
        if path is not None:
            result.to_file(path, custom_evaluator_types=custom_evaluator_types)
        return result
      
  
---|---  
  
© Pydantic Services Inc. 2024 to present

