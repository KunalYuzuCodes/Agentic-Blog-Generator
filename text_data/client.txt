Skip to content

[ ![logo](../../img/logo-white.svg) ](../.. "PydanticAI")

PydanticAI

Client

Type to start searching

[ pydantic/pydantic-ai

  * v0.0.50
  * 8k
  * 687

](https://github.com/pydantic/pydantic-ai "Go to repository")

[ ![logo](../../img/logo-white.svg) ](../.. "PydanticAI") PydanticAI

[ pydantic/pydantic-ai

  * v0.0.50
  * 8k
  * 687

](https://github.com/pydantic/pydantic-ai "Go to repository")

  * [ Introduction  ](../..)
  * [ Installation  ](../../install/)
  * [ Getting Help  ](../../help/)
  * [ Contributing  ](../../contributing/)
  * [ Troubleshooting  ](../../troubleshooting/)
  * Documentation  Documentation 
    * [ Agents  ](../../agents/)
    * [ Models  ](../../models/)
    * [ Dependencies  ](../../dependencies/)
    * [ Function Tools  ](../../tools/)
    * [ Common Tools  ](../../common-tools/)
    * [ Results  ](../../results/)
    * [ Messages and chat history  ](../../message-history/)
    * [ Unit testing  ](../../testing/)
    * [ Debugging and Monitoring  ](../../logfire/)
    * [ Multi-agent Applications  ](../../multi-agent-applications/)
    * [ Graphs  ](../../graph/)
    * [ Evals  ](../../evals/)
    * [ Image, Audio & Document Input  ](../../input/)
    * [ MCP  ](../)

MCP

      * Client  [ Client  ](./) Table of contents 
        * Install 
        * Usage 
          * SSE Client 
          * MCP "stdio" Server 
      * [ Server  ](../server/)
      * [ MCP Run Python  ](../run-python/)
    * [ Command Line Interface (CLI)  ](../../cli/)
  * [ Examples  ](../../examples/)

Examples

    * [ Pydantic Model  ](../../examples/pydantic-model/)
    * [ Weather agent  ](../../examples/weather-agent/)
    * [ Bank support  ](../../examples/bank-support/)
    * [ SQL Generation  ](../../examples/sql-gen/)
    * [ Flight booking  ](../../examples/flight-booking/)
    * [ RAG  ](../../examples/rag/)
    * [ Stream markdown  ](../../examples/stream-markdown/)
    * [ Stream whales  ](../../examples/stream-whales/)
    * [ Chat App with FastAPI  ](../../examples/chat-app/)
    * [ Question Graph  ](../../examples/question-graph/)
  * API Reference  API Reference 
    * [ pydantic_ai.agent  ](../../api/agent/)
    * [ pydantic_ai.tools  ](../../api/tools/)
    * [ pydantic_ai.common_tools  ](../../api/common_tools/)
    * [ pydantic_ai.result  ](../../api/result/)
    * [ pydantic_ai.messages  ](../../api/messages/)
    * [ pydantic_ai.exceptions  ](../../api/exceptions/)
    * [ pydantic_ai.settings  ](../../api/settings/)
    * [ pydantic_ai.usage  ](../../api/usage/)
    * [ pydantic_ai.mcp  ](../../api/mcp/)
    * [ pydantic_ai.format_as_xml  ](../../api/format_as_xml/)
    * [ pydantic_ai.models  ](../../api/models/base/)
    * [ pydantic_ai.models.openai  ](../../api/models/openai/)
    * [ pydantic_ai.models.anthropic  ](../../api/models/anthropic/)
    * [ pydantic_ai.models.bedrock  ](../../api/models/bedrock/)
    * [ pydantic_ai.models.cohere  ](../../api/models/cohere/)
    * [ pydantic_ai.models.gemini  ](../../api/models/gemini/)
    * [ pydantic_ai.models.groq  ](../../api/models/groq/)
    * [ pydantic_ai.models.instrumented  ](../../api/models/instrumented/)
    * [ pydantic_ai.models.mistral  ](../../api/models/mistral/)
    * [ pydantic_ai.models.test  ](../../api/models/test/)
    * [ pydantic_ai.models.function  ](../../api/models/function/)
    * [ pydantic_ai.models.fallback  ](../../api/models/fallback/)
    * [ pydantic_ai.models.wrapper  ](../../api/models/wrapper/)
    * [ pydantic_ai.providers  ](../../api/providers/)
    * [ pydantic_graph  ](../../api/pydantic_graph/graph/)
    * [ pydantic_graph.nodes  ](../../api/pydantic_graph/nodes/)
    * [ pydantic_graph.persistence  ](../../api/pydantic_graph/persistence/)
    * [ pydantic_graph.mermaid  ](../../api/pydantic_graph/mermaid/)
    * [ pydantic_graph.exceptions  ](../../api/pydantic_graph/exceptions/)
    * [ pydantic_evals.dataset  ](../../api/pydantic_evals/dataset/)
    * [ pydantic_evals.evaluators  ](../../api/pydantic_evals/evaluators/)
    * [ pydantic_evals.reporting  ](../../api/pydantic_evals/reporting/)
    * [ pydantic_evals.otel  ](../../api/pydantic_evals/otel/)
    * [ pydantic_evals.generation  ](../../api/pydantic_evals/generation/)

Table of contents

  * Install 
  * Usage 
    * SSE Client 
    * MCP "stdio" Server 

# Client

PydanticAI can act as an [MCP
client](https://modelcontextprotocol.io/quickstart/client), connecting to MCP
servers to use their tools.

## Install

You need to either install [`pydantic-ai`](../../install/), or[`pydantic-ai-
slim`](../../install/#slim-install) with the `mcp` optional group:

pipuv

    
    
    pip install "pydantic-ai-slim[mcp]"
    
    
    
    uv add "pydantic-ai-slim[mcp]"
    

Note

MCP integration requires Python 3.10 or higher.

## Usage

PydanticAI comes with two ways to connect to MCP servers:

  * [`MCPServerHTTP`](../../api/mcp/#pydantic_ai.mcp.MCPServerHTTP) which connects to an MCP server using the [HTTP SSE](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) transport
  * [`MCPServerStdio`](../../api/mcp/#pydantic_ai.mcp.MCPServerStdio) which runs the server as a subprocess and connects to it using the [stdio](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) transport

Examples of both are shown below; [mcp-run-python](../run-python/) is used as
the MCP server in both examples.

### SSE Client

[`MCPServerHTTP`](../../api/mcp/#pydantic_ai.mcp.MCPServerHTTP) connects over
HTTP using the [HTTP + Server Sent Events
transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-
with-sse) to a server.

Note

[`MCPServerHTTP`](../../api/mcp/#pydantic_ai.mcp.MCPServerHTTP) requires an
MCP server to be running and accepting HTTP connections before calling
[`agent.run_mcp_servers()`](../../api/agent/#pydantic_ai.agent.Agent.run_mcp_servers).
Running the server is not managed by PydanticAI.

The name "HTTP" is used since this implemented will be adapted in future to
use the new [Streamable
HTTP](https://github.com/modelcontextprotocol/specification/pull/206)
currently in development.

Before creating the SSE client, we need to run the server (docs [here](../run-
python/)):

terminal (run sse server)

    
    
    npx @pydantic/mcp-run-python sse
    

mcp_sse_client.py

    
    
    from pydantic_ai import Agent
    from pydantic_ai.mcp import MCPServerHTTP
    
    server = MCPServerHTTP(url='http://localhost:3001/sse')  
    agent = Agent('openai:gpt-4o', mcp_servers=[server])  
    
    
    async def main():
        async with agent.run_mcp_servers():  
            result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')
        print(result.data)
        #> There are 9,208 days between January 1, 2000, and March 18, 2025.
    

  1.   2.   3. 

_(This example is complete, it can be run "as is" with Python 3.10+ — you'll
need to add`asyncio.run(main())` to run `main`)_

**What's happening here?**

  * The model is receiving the prompt "how many days between 2000-01-01 and 2025-03-18?"
  * The model decides "Oh, I've got this `run_python_code` tool, that will be a good way to answer this question", and writes some python code to calculate the answer.
  * The model returns a tool call
  * PydanticAI sends the tool call to the MCP server using the SSE transport
  * The model is called again with the return value of running the code
  * The model returns the final answer

You can visualise this clearly, and even see the code that's run by adding
three lines of code to instrument the example with
[logfire](https://logfire.pydantic.dev/docs):

mcp_sse_client_logfire.py

    
    
    import logfire
    
    logfire.configure()
    logfire.instrument_pydantic_ai()
    

Will display as follows:

[![Logfire run python code](../../img/logfire-run-python-
code.png)](../../img/logfire-run-python-code.png)

### MCP "stdio" Server

The other transport offered by MCP is the [stdio
transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio)
where the server is run as a subprocess and communicates with the client over
`stdin` and `stdout`. In this case, you'd use the
[`MCPServerStdio`](../../api/mcp/#pydantic_ai.mcp.MCPServerStdio) class.

Note

When using [`MCPServerStdio`](../../api/mcp/#pydantic_ai.mcp.MCPServerStdio)
servers, the
[`agent.run_mcp_servers()`](../../api/agent/#pydantic_ai.agent.Agent.run_mcp_servers)
context manager is responsible for starting and stopping the server.

mcp_stdio_client.py

    
    
    from pydantic_ai import Agent
    from pydantic_ai.mcp import MCPServerStdio
    
    server = MCPServerStdio('npx', ['-y', '@pydantic/mcp-run-python', 'stdio'])
    agent = Agent('openai:gpt-4o', mcp_servers=[server])
    
    
    async def main():
        async with agent.run_mcp_servers():
            result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')
        print(result.data)
        #> There are 9,208 days between January 1, 2000, and March 18, 2025.
    

© Pydantic Services Inc. 2024 to present

